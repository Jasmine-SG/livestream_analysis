{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Passion to Profit: Exploring Emotions as Indicators of Monetized Message Engagement\n",
    "Jasmine Guo \\\n",
    "Data Science Institute \\\n",
    "DS 5780 Natural Language Processing \\\n",
    "Dr. Scott Crossley \\\n",
    "April 23, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing \n",
    "This notebook is an auxiliary notebook to the main notebook. The purpose of this notebook is to process the data to a dataframe, which will be used in the main notebook. The main notebook will contain more data wranling, but those wrangling are specific to the main notebook, and thus it's kept there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process in this notebook is as follows:\n",
    "- Reading all the data from local drive.\n",
    "- Binding all the data into a single dataframe.\n",
    "- Filtering the data to only include English comments, and comments are actually messages.\n",
    "- Save the dataframe into a .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the library \n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by loading in the data. The data are stored in the subdirectory of the working directory. The regular message are part of the \"regular\" folder, and the monetized mesaages are part of the \"superchat\" folder, taking name from YouTube's naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "\n",
    "# folder path\n",
    "regularFP = \"data/regular/\"\n",
    "monetizedFP = \"data/superchat/\"\n",
    "\n",
    "# list of dataframes\n",
    "regularDF = []\n",
    "monetizedDF = []\n",
    "\n",
    "# looping through the directory, extract the file and append it to the respective list\n",
    "for filename in os.listdir(regularFP):\n",
    "    file_path = os.path.join(regularFP, filename)\n",
    "    df = pd.read_table(file_path, header = None)\n",
    "    regularDF.append(df) \n",
    "\n",
    "for filename in os.listdir(monetizedFP):\n",
    "    file_path = os.path.join(monetizedFP, filename) \n",
    "    df = pd.read_table(file_path, header = None)\n",
    "    monetizedDF.append(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the data into one dataframe\n",
    "regularDF = pd.concat(regularDF)\n",
    "monetizedDF = pd.concat(monetizedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will split the data into its respective columns. The data are stored in a .txt file without consistent format, and thus were loaded in as a single string. \\\n",
    "Between the timestampe and name column, the deliminater is \"|\", and between the name and the comment column, the deliminater is a colon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the regular data into timestamp, name and comment\n",
    "# using n = 1 to split only upon first occurance\n",
    "timeStampDF = regularDF[0].str.split(\" | \", expand=True, n=1)\n",
    "commentDF = timeStampDF[1].str.split(\":\", expand=True, n=1)\n",
    "regularSplitDF = pd.concat([timeStampDF[0], commentDF], axis=1)\n",
    "regularSplitDF.columns = ['Timestamp', 'Name', 'Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the monetized data into timestamp, name and comment\n",
    "# using n = 1 to split only upon first occurance\n",
    "timeStampDF = monetizedDF[0].str.split(\"|\", expand=True, n=1)\n",
    "commentDF = timeStampDF[1].str.split(\":\", expand=True, n=1)\n",
    "monetizedSplitDF = pd.concat([timeStampDF[0], commentDF], axis=1)\n",
    "monetizedSplitDF.columns = ['Timestamp', 'Name', 'Comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will further process the data to remove null values and system notification of memberships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing NaN values\n",
    "regularSplitDF.dropna(subset = [\"Comment\"], inplace=True)\n",
    "monetizedSplitDF.dropna(subset = [\"Comment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing gifted message\n",
    "monetizedSplitDF = monetizedSplitDF[~monetizedSplitDF['Comment'].str.contains('Gifted')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of regular comments are: 1381915\n",
      "The number of monetized comments are: 6001\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of regular comments are: {len(regularSplitDF)}\")\n",
    "print(f\"The number of monetized comments are: {len(monetizedSplitDF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently over 1M regular comments and only 6k monetized comments. We will subset the regular message to 20k to reduce the scope for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularSplitDFS = regularSplitDF.sample(n=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now filter out non-English comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"Error\"\n",
    "\n",
    "# detect comment langauge and keep only english comments\n",
    "regularSplitDFS['Language'] = regularSplitDFS['Comment'].apply(detect_language)\n",
    "monetizedSplitDF['Language'] = monetizedSplitDF['Comment'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularEN = regularSplitDFS[regularSplitDFS['Language'] == 'en']\n",
    "monetizedEN = monetizedSplitDF[monetizedSplitDF['Language'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now save the dataframe into a .csv file. The file will be further wrangled for the used wihtin the main notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularEN.to_csv('data/regularEN.csv', index=False)\n",
    "monetizedEN.to_csv('data/monetizedEN.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds5780NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
